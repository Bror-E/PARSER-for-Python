{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSER: A model for word segmentation\n",
    "## A rough guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start with all of the important imports that we will be using in this notebook\n",
    "    For some reason the sys.path.append does not want to import the PARSER module so we have to import it like this. And we also define two folder holding some datasets. We also create a varaible called parameters, so that we can turn of logging of the model. You can turn this on or off as you please by modifying the parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "root_folder = os.getcwd()\n",
    "PARSER_folder = os.path.join(root_folder,\"PARSER\")\n",
    "sys.path.append(PARSER_folder)\n",
    "\n",
    "import PARSER\n",
    "import PARSER_Tester\n",
    "import data_handeling\n",
    "\n",
    "from PARSER_class import PARSER\n",
    "\n",
    "data_folder_gabriel = os.path.join(root_folder,\"Data\",\"Gabriel Original Datasets\")\n",
    "data_folder_parser = os.path.join(root_folder,\"Data\",\"PARSER Pre-saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For simplicty and consistency in this tutorial, we will use a seed for the random-number generator in the PARSER model. \n",
    "#If you wish, you can change the parameters at this stage to f.ex support logging, larger percept sizes and so and so forth. \n",
    "parameters = PARSER.get_default_parameters()\n",
    "parameters['random_seed'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What will we run the model on. Corpus creation.\n",
    "    The PARSER model is created to run any type of string input. You simply create a string and feed it to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to create a simple string and feed it to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string_simple = \"ababababababababbbabbabbabbabbabba\"\n",
    "PARSER.run(input_string_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to create a simple string from one of Gabriels datasets and from the datasets that are premade in the original PARSER model (The PARSER.exe programme). \n",
    "For simplicity we create two dictionaries containing all of the names of the papers in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_titles_gabriel = {i:title for i,title in enumerate(data_handeling.get_paper_names(data_folder_gabriel))}\n",
    "print(paper_titles_gabriel)\n",
    "paper_titles_parser = {i:title for i,title in enumerate(data_handeling.get_paper_names(data_folder_parser))}\n",
    "print(paper_titles_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have multiple datasets we can choose from! This next example showcases how we can extract the data that is in one of these datasets. This is rather simple and we only use the data_handeling class to do this. Here we will work with the 2'nd dataset from the list of gabriels papers. In all of these examples we are directly loading the information from the datasets, and not using the variables who hold the data.  \n",
    "1. First we show how you get all the data from a dataset.\n",
    "2. We show how to display the categories of the dataset.\n",
    "3. Shows how to get the strings from a certain category in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_title = paper_titles_gabriel.get(1)\n",
    "paper_data = data_handeling.get_paper_data(data_folder_gabriel,paper_title)\n",
    "#paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_categories = data_handeling.get_paper_categories(data_folder_gabriel, paper_title)\n",
    "#paper_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the code above, we know that the cateogies in that dataset are [Exposure, Test, Consistent, Violating].\n",
    "#We want to use the exposure strings for training a model. Therefore we only extract these. \n",
    "paper_categories = data_handeling.get_paper_categories(data_folder_gabriel, paper_title)\n",
    "paper_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_category_strings = data_handeling.get_paper_strings(data_folder_gabriel, paper_title, ['Exposure'])\n",
    "#paper_category_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the model, we need to generate input for the model. This can be done using the data_handeling class as well. The method \"generate_input_data()\" takes two parameters. The string category you want to train it on (in the form of a dictionary: 'category':[strings]}) and the number of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_items = 200\n",
    "input_string_example = data_handeling.generate_input_data(paper_category_strings, number_of_items)\n",
    "input_string_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our notebook however, I want things to be consistent so that one can follow anlong all the way. So same as with the random-seed we wanted to be consistent, we want the input to be consistent as well. Therefore I add an input string here that is constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = 'acgfcgacgfacfcgacgfacgfcgadcfacfcgadcfcacgfcgacfcadcfadcfcacgfadcfcacgfcgadcfadcgfcacfcadcfcadcfadcfcadcfadcfcgacgfcgacfcgacfcacgfcgadcfadcfcgadcfadcgfcadcfcacfcgadcgfcacgfcgadcgfcadcfcgacgfcgadcgfcacgfacgfcgadcfcgacfcadcfcgacgfcgadcfcgadcfcadcgfcadcfadcfcgadcgfcadcfcgacfcadcfcadcgfcacfcgacfcadcgfcacfcacgfacfcgadcfcgacfcgadcfcadcfcgacgfadcfcadcgfcacgfacfcgadcfcacgfadcfcacfcadcfadcgfcacfcadcfadcfcgadcfcgacfcgacgfacgfcgacgfcgadcgfcacfcacgfcgadcfadcfacgfacfcacfcacfcgadcgfcadcfacgfadcfcacgfadcfadcfadcfacgfcgacfcadcfcacgfcgadcgfcacgfcgacfcacgfacgfcgacfcacgfcgacgfacfcacgfacgfcgadcgfcacfcgadcfcadcfcgacgfcgacgfcgacgfacfcgadcfcgacgfacgfcgadcfadcfcadcfacfcacgfacgfcgadcfcacgfcgadcgfcacgfcgacgfcgadcfacfcgadcfadcfacgfacfcadcfadcgfcacfcgacfcadcgfcadcfadcgfcadcfcacfcadcfcadcfcacfcacfcgadcfcacgfcgadcfcadcfadcfcadcfcgacgfacfcadcfacfcgadcfcacgfadcfacgfadcfcacgfadcfcgacfcacfcgadcfcacgfadcgfcadcfcacfcgadcfacgfadcfacgfcgadcfcacgfadcfadcfcadcfacgfcgacgfadcgfcadcfcgadcfcgadcfcgacfcacfcgadcfadcf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training\n",
    "Now that we have created an input string that is made up from the exposure strings of a dataset. We are ready to feed the model this string to see if it can learn the chunks/grammar. The return from the PARSER_Trainer is a triplet that is unpacked to be: The percept_shaper, the primitives and the parameters. One can avoid the triple return by running it directly with the PARSER.run function. The PARSER_trainer is mostly in use if one wants to save and run multiple runs and stuff. It is important here that if you do not want to use random seeds for each created model, that you make a parameters variable and use this consistently (the first time you run it, the random-seed will save itself as a entry in the parameters. Causing that seed to be re-used again in later runs with the same parameter. Or you could manually set a random-seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, primitives, _ = PARSER.train(input_string, parameters= parameters)\n",
    "trained_model\n",
    "\n",
    "#trained_model = PARSER.run(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing\n",
    "Now that we have a trained model. We can test the trained model to see how well it has learned the grammar that we tried to teach it. We can ask the model if a single word i consitent with the grammar the model as learned, or we can ask if a set of strings are consistent. So lets first get the testing string set from the data. Then test the model on a single string from this set, and then on the entire set. Normally, a trained model should have a function to test it self on input. Something along the lines of \"trained_model.predict(input)\" and then you get a bool back or something. This is not the case here. And the reason for that is that a trained model here is no more than a list of touples consisting of a chunk and its weight, which does not deserve its own object. Therefore for simplicity I made it as a dictionary. And the testing function is made separtly for multiple reasons. 1. The analysis of the model can be done in multiple ways. My way is just one of many, having therefore a separate test class makes sense. 2. The goal is to have many different models, not just PARSER, to output similar results. This means that all of these models would have to have the same testing function within them, which is not necessary if you can have it outside the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input: \n",
    "testing_data = data_handeling.get_paper_strings(data_folder_gabriel, paper_title, ['Test', 'Consistent','Violating'])\n",
    "single__test_input = testing_data.get('Consistent')[1]\n",
    "PARSER_Tester.give_single_input_to_model(trained_model, primitives, single__test_input, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiple inputs: \n",
    "multiple_test_input = testing_data.get('Test')\n",
    "PARSER_Tester.test_all_string_in_list(trained_model,primitives,multiple_test_input,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now lets test the strings that in theory is consistent with the grammar. (Answer should be 4)\n",
    "consistent_test_input = testing_data.get('Consistent')\n",
    "PARSER_Tester.test_all_string_in_list(trained_model,primitives,consistent_test_input,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then lets test the string that are supposedly in violation with the grammar. (Answer should be 1)\n",
    "violation_test_input = testing_data.get('Violating')\n",
    "PARSER_Tester.test_all_string_in_list(trained_model,primitives,violation_test_input,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analytics\n",
    "Here is an explanation and exploration of analaytical methods to analyse and compare models. For simplicty, we use normal machine learning measurments to calculate the various score of our model. A deep explenation of the testing procedures (how we count a string as consistent or violating can be read __elsewhere__)\n",
    "\n",
    "Here is the explanation of the various measurements that is being done:\n",
    "\t- True positive   = A consistent string is classified as consistent\n",
    "    - True negative   = A violating string classified as violating\n",
    "\t- False positive  = A violoating string classified as consistent\n",
    "\t- False negative  = A consisten string classified as violating \n",
    "\n",
    "\n",
    "TODO: \n",
    "Make a method that gets all of the possible A->B combinations of a trainingset and the consistent set. \n",
    "                  training + testin set (possible)  = number of possible learning nodes. \n",
    "the number of unique A->B's in the model  (learnt)  = number of learnt nodes\n",
    "\n",
    "learnt/possible = the percentage of learnt nodes. \n",
    "\n",
    "Ok, this works now. Trying to plot it tho and it looks kinda strange. But it works. It's the methods main3 in the temp_main script. Ofcourse re-organize it, and then put into parser_trainer, or parser_tester or something like that.\n",
    "\n",
    "\n",
    "Okei, plotly is fucking fantastic. But the rpoblem becomes the random-number generator. How to keep this consistent over multiple runs if we restart the model every time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
